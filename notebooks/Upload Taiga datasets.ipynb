{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from taigapy import TaigaClient\n",
    "\n",
    "#PARAMETERS\n",
    "min_avg_Geff = 0.2\n",
    "min_sum_Geff = 1.5\n",
    "min_hps_per_gene = 3\n",
    "\n",
    "c = TaigaClient()\n",
    "\n",
    "base_data_dir = '/Users/jmmcfarl/CPDS/demeter2'\n",
    "\n",
    "Ach_model_dir = os.path.join(base_data_dir,'kube_results/Ach_final/1')\n",
    "Ach_dataset_id = 'demeter2-achilles-5386'\n",
    "\n",
    "DRIVE_model_dir = os.path.join(base_data_dir, 'kube_results/DRIVE_final/1')\n",
    "DRIVE_dataset_id = 'demeter2-drive-0591'\n",
    "\n",
    "Marc_model_dir = os.path.join(base_data_dir, 'kube_results/Marc_fix/1')\n",
    "Marc_dataset_id = 'demeter2-marcotte-a703'\n",
    "\n",
    "comb_model_dir = os.path.join(base_data_dir, 'kube_results/comb_fix/1')\n",
    "comb_dataset_id = 'demeter2-combined-dc9c'\n",
    "\n",
    "# new_name_map = pd.read_csv('/Users/jmmcfarl/CPDS/demeter2/results/name_change_map.csv')\n",
    "# new_name_map_dict = {a: b for a, b in zip(new_name_map.old_name, new_name_map.new_name)}\n",
    "\n",
    "# name_correction_dict = pd.read_csv('/Users/jmmcfarl/CPDS/demeter2/data/CCLE_name_corrections.csv')\n",
    "# name_correction_dict = {a: b for a, b in zip(name_correction_dict.old_name, name_correction_dict.new_name)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hart_ess = c.get(name='demeter2-pos-neg-controls-a5c6', version=1, file='hart_pos_controls')['Gene_ID'].values.astype(str)\n",
    "hart_non_ess = c.get(name='demeter2-pos-neg-controls-a5c6', version=1, file='hart_neg_controls')['Gene_ID'].values.astype(str)\n",
    "\n",
    "sh_targets = c.get(name = 'gpp-shrna-mapping-8759', version = 6, file = 'shmap_19mer_noXLOC_Entrezonly')\n",
    "sh_targets = sh_targets.rename(columns = {'Barcode Sequence': 'hp'}, inplace = False).set_index('hp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "gene_name_df = sh_targets.ix[~sh_targets['Gene ID'].str.contains('^NO_CURRENT')].copy()\n",
    "gene_name_df['Gene name'] = gene_name_df['Gene Symbol'] + ' (' + gene_name_df['Gene ID'] + ')'\n",
    "gene_name_map = gene_name_df.set_index('Gene ID', inplace = False)['Gene name']\n",
    "gene_name_map = gene_name_map.to_dict()\n",
    "\n",
    "gene_sym_map = gene_name_df.set_index('Gene ID', inplace = False)['Gene Symbol']\n",
    "gene_sym_map = gene_sym_map.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D2_description='''\n",
    "Results from DEMETER2 model fit.\n",
    "\n",
    "Contents:\n",
    "\n",
    "* gene_means_proc: posterior mean estimates of essentiality for each gene/CL pair\n",
    "* gene_SDs_proc: posterior SD of essentiality estimates for each gene/CL pair  \n",
    "* hp_data_comb: model results for each hairpin, including:\n",
    "    * Geff: hairpin gene knockdown efficacy [0,1]\n",
    "    * Seff: hairpin seed knockdown efficacy [0,1]\n",
    "    * unpred_offset_mean: posterior mean of across-CL avg unpredicted offtarget effect\n",
    "    * unpred_offset_sd: posterior SD of across-CL avg unpredicted offtarget effect\n",
    "* CL_data_comb: model results for each CL, including:\n",
    "    * gene_slope: RNAi efficacy parameter\n",
    "    * CL_slope: overall scaling factor\n",
    "    * noise_vars: noise variance\n",
    "    * offset_mean: posterior mean of additive offset\n",
    "    * offset_SD: posterior SD of additive offset\n",
    "    \n",
    "versions:\n",
    "* v2: \n",
    "    * Run with final hyperparameter settings\n",
    "    * Add gene symbols to entrez IDs\n",
    "    * exclude genes with all NA values\n",
    "    * exclude genes with poor reagents\n",
    "    * normalize gene scores to have median of pos-cons at -1 and median of neg-cons at 0\n",
    "\n",
    "* v3: \n",
    "    * Add gene symbols for gene families\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_matrix(path):\n",
    "    mat = pd.read_csv(path)\n",
    "    mat.iloc[:,0] = mat.iloc[:,0].astype(str)\n",
    "    mat.set_index(mat.columns[0], inplace=True)\n",
    "    return(mat)\n",
    "    \n",
    "\n",
    "def make_processed_gene_data_D2(cur_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff):\n",
    "    '''Process gene means and SDs and make new files'''\n",
    "#     gs = pd.read_csv(os.path.join(cur_model_dir, 'gene_means.csv'), index_col = 0)\n",
    "#     gs_unc = pd.read_csv(os.path.join(cur_model_dir, 'gene_SDs.csv'), index_col = 0)\n",
    "    gs = load_matrix(os.path.join(cur_model_dir, 'gene_means.csv'))\n",
    "    gs_unc = load_matrix(os.path.join(cur_model_dir, 'gene_SDs.csv'))\n",
    "\n",
    "    ss = load_matrix(os.path.join(cur_model_dir, 'seed_means.csv'))\n",
    "    ss_unc = load_matrix(os.path.join(cur_model_dir, 'seed_SDs.csv'))\n",
    "\n",
    "    \n",
    "#     #update cell line names\n",
    "#     gs.columns = gs.columns.to_series().replace(new_name_map_dict)\n",
    "#     gs_unc.columns = gs_unc.columns.to_series().replace(new_name_map_dict)\n",
    "    \n",
    "#     #fix CCLE name problems\n",
    "#     gs.columns = gs.columns.to_series().replace(name_correction_dict)\n",
    "#     gs_unc.columns = gs_unc.columns.to_series().replace(name_correction_dict)\n",
    "    \n",
    "#     #drop non Entrez genes\n",
    "#     gs = gs.filter(regex = '[0-9]', axis = 0)\n",
    "#     gs_unc = gs_unc.filter(regex = '[0-9]', axis = 0)\n",
    "    \n",
    "    #drop genes which are NA for all cell lines\n",
    "    bad_genes = np.where(gs.isnull().sum(axis = 1) == gs.shape[1])[0]\n",
    "    print('Removing {} genes with all NAs'.format(len(bad_genes)))\n",
    "    gs.drop(gs.index[bad_genes], axis=0, inplace=True)\n",
    "    gs_unc.drop(gs_unc.index[bad_genes], axis=0, inplace=True)\n",
    "\n",
    "    #calc mean Geff and sum Geff per gene, and filter out bad-quality genes\n",
    "    hp_data = pd.read_csv(os.path.join(cur_model_dir, 'hp_data.csv')).set_index('hp')\n",
    "    hp_data = hp_data.join(sh_targets, how = 'left')\n",
    "    hp_stats = hp_data.groupby('Gene ID').agg({'Geff': [np.mean, np.sum, 'count']})\n",
    "    bad_genes = hp_stats.ix[(hp_stats['Geff']['mean'].values < min_avg_Geff) | (hp_stats['Geff']['sum'].values < min_sum_Geff) | \\\n",
    "                            (hp_stats['Geff']['count'].values < min_hps_per_gene)].index.values\n",
    "    bad_genes = np.intersect1d(bad_genes, gs.index.values)\n",
    "    print('Removing {} genes with all poor hp data'.format(len(bad_genes)))\n",
    "    gs.drop(bad_genes, axis=0, inplace=True)\n",
    "    gs_unc.drop(bad_genes, axis=0, inplace=True)\n",
    "\n",
    "    #normalize gene scores by pos-neg control medians\n",
    "    weights = 1/(gs_unc**2)\n",
    "    per_gene_avgs = np.sum(gs * weights, axis = 1) / np.sum(weights, axis = 1)\n",
    "    pos_con_median = np.nanmedian(per_gene_avgs.ix[hart_ess])\n",
    "    neg_con_median = np.nanmedian(per_gene_avgs.ix[hart_non_ess])\n",
    "\n",
    "    norm_gs_unc = gs_unc / (neg_con_median - pos_con_median)\n",
    "    norm_gs = (gs - neg_con_median) / (neg_con_median - pos_con_median)\n",
    "    #apply same rescaling to seed scores\n",
    "    norm_ss_unc = ss_unc / (neg_con_median - pos_con_median)\n",
    "    norm_ss = (ss - neg_con_median) / (neg_con_median - pos_con_median)\n",
    "\n",
    "    #rename genes to include gene symbol\n",
    "    norm_gs.rename(index = gene_name_map, inplace = True)\n",
    "    norm_gs_unc.rename(index = gene_name_map, inplace = True)\n",
    "    \n",
    "    #handle gene names for gene families\n",
    "    gene_families = np.where(norm_gs.index.str.contains('&', na = False))[0]\n",
    "    ind_names = norm_gs.index.values\n",
    "    for fam_ind in gene_families:\n",
    "        cur_fam = norm_gs.index.values[fam_ind]\n",
    "#         print(cur_fam)\n",
    "        fam_syms = '&'.join([gene_sym_map[x] for x in re.split('&', cur_fam)])\n",
    "#         ind_names[fam_ind] = cur_fam + ' (' + fam_syms + ')'\n",
    "        ind_names[fam_ind] = fam_syms + ' (' + cur_fam + ')'\n",
    "    norm_gs.index = ind_names\n",
    "    norm_gs_unc.index = ind_names\n",
    "\n",
    "    norm_gs.to_csv(os.path.join(cur_model_dir, 'gene_means_proc.csv'))\n",
    "    norm_gs_unc.to_csv(os.path.join(cur_model_dir, 'gene_SDs_proc.csv'))\n",
    "    norm_ss.to_csv(os.path.join(cur_model_dir, 'seed_means_proc.csv'))\n",
    "    norm_ss_unc.to_csv(os.path.join(cur_model_dir, 'seed_SDs_proc.csv'))\n",
    "\n",
    "    \n",
    "def prepare_D2_outputs(D2_model_dir):\n",
    "    '''Combine batch and non-batch parameters for CL data and hp_data'''\n",
    "#     CL_data = pd.read_csv(os.path.join(D2_model_dir, 'CL_data.csv'), index_col = 0)\n",
    "    CL_data = load_matrix(os.path.join(D2_model_dir, 'CL_data.csv'))\n",
    "    \n",
    "#     CL_batch_data = pd.read_csv(os.path.join(D2_model_dir, 'CL_batch_data.csv'), index_col = 0)\n",
    "    CL_batch_data = load_matrix(os.path.join(D2_model_dir, 'CL_batch_data.csv'))\n",
    "   \n",
    "    #rename cell lines\n",
    "#     CL_data.index = CL_data.index.to_series().replace(new_name_map_dict)\n",
    "#     CL_batch_data.index = CL_batch_data.index.to_series().replace(new_name_map_dict)\n",
    " \n",
    "#     #fix CCLE name problems\n",
    "#     CL_data.index = CL_data.index.to_series().replace(name_correction_dict)\n",
    "#     CL_batch_data.index = CL_batch_data.index.to_series().replace(name_correction_dict)\n",
    "\n",
    "    CL_batch_data.reset_index(inplace=True)\n",
    "    CL_batch_data['offset_var'] = CL_batch_data['offset_sd']**2\n",
    "    CL_batch_means = CL_batch_data.groupby('CCLE_ID')[['CL_slope', 'noise_vars', 'offset_mean', 'offset_var']].agg('mean')\n",
    "    CL_batch_means['offset_sd'] = np.sqrt(CL_batch_means['offset_var'].values)\n",
    "\n",
    "    CL_data = pd.merge(CL_data, CL_batch_means[['CL_slope', 'noise_vars', 'offset_mean', 'offset_sd']], left_index=True, right_index = True)\n",
    "    CL_data.to_csv(os.path.join(D2_model_dir, 'CL_data_comb.csv'))\n",
    "    \n",
    "    hp_data = pd.read_csv(os.path.join(D2_model_dir, 'hp_data.csv')).set_index('hp')\n",
    "    hp_batch_data = pd.read_csv(os.path.join(D2_model_dir, 'hp_batch_data.csv')).reset_index()\n",
    "    hp_batch_data['hairpin_offset_var'] = hp_batch_data['hairpin_offset_sd']**2\n",
    "    hp_batch_means = hp_batch_data.groupby('hp')[['hairpin_offset_mean', 'hairpin_offset_var']].agg('mean')\n",
    "    hp_batch_means['hairpin_offset_sd'] = np.sqrt(hp_batch_means['hairpin_offset_var'].values)\n",
    "\n",
    "    hp_data = pd.merge(hp_data[['Geff', 'Seff', 'unpred_offset_mean', 'unpred_offset_sd']], hp_batch_means[['hairpin_offset_sd', 'hairpin_offset_mean']], left_index=True, right_index = True)\n",
    "    hp_data.to_csv(os.path.join(D2_model_dir, 'hp_data_comb.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Achilles data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2869: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 387 genes with all NAs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:41: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 358 genes with all poor hp data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:50: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:50: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:51: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:51: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now choosing the datasets you would want to keep or remove:\n",
      "\tKeep gene_dependency ? (y/n) y\n",
      "\tKeep gene_SDs_proc ? (y/n) y\n",
      "\tKeep pan_dependent_genes ? (y/n) y\n",
      "\tKeep gene_effect ? (y/n) y\n",
      "\tKeep gene_means_proc ? (y/n) y\n",
      "\tKeep CL_data_comb ? (y/n) y\n",
      "\tKeep gene_fdr ? (y/n) y\n",
      "\tKeep hp_data_comb ? (y/n) y\n",
      "Uploading seed_means_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 7001)\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 4000\n",
      "\t Conversion in progress, line 15000\n",
      "\t Conversion in progress, line 15000\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: seed_means_proc properly converted and uploaded\n",
      "Uploading seed_SDs_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 9001)\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 14000\n",
      "\t Conversion in progress, line 15000\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: seed_SDs_proc properly converted and uploaded\n",
      "Creating the new version with these files:\n",
      "\tNEW: seed_means_proc - NumericMatrixCSV\n",
      "\tNEW: seed_SDs_proc - NumericMatrixCSV\n",
      "\tKEEP: gene_dependency - HDF5\n",
      "\tKEEP: gene_SDs_proc - HDF5\n",
      "\tKEEP: pan_dependent_genes - Columnar\n",
      "\tKEEP: gene_effect - HDF5\n",
      "\tKEEP: gene_means_proc - HDF5\n",
      "\tKEEP: CL_data_comb - HDF5\n",
      "\tKEEP: gene_fdr - HDF5\n",
      "\tKEEP: hp_data_comb - HDF5\n",
      "\n",
      "Dataset version with id e7fc2923276d4282936009b5d9da73c3 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/e7fc2923276d4282936009b5d9da73c3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'e7fc2923276d4282936009b5d9da73c3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_processed_gene_data_D2(Ach_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff)\n",
    "prepare_D2_outputs(Ach_model_dir)\n",
    "\n",
    "c.update_dataset(dataset_permaname=Ach_dataset_id,\n",
    "#     dataset_description=D2_description,\n",
    "#     upload_file_path_dict={os.path.join(Ach_model_dir, 'CL_data_comb.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(Ach_model_dir, 'hp_data_comb.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(Ach_model_dir, 'gene_means_proc.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(Ach_model_dir, 'gene_SDs_proc.csv'): 'NumericMatrixCSV'})\n",
    "    upload_file_path_dict={os.path.join(Ach_model_dir, 'seed_means_proc.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(Ach_model_dir, 'seed_SDs_proc.csv'): 'NumericMatrixCSV'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRIVE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 646 genes with all NAs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:41: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:50: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:50: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:51: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:51: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 568 genes with all poor hp data\n",
      "Now choosing the datasets you would want to keep or remove:\n",
      "\tKeep CL_data_comb ? (y/n) y\n",
      "\tKeep gene_fdr ? (y/n) y\n",
      "\tKeep pan_dependent_genes ? (y/n) y\n",
      "\tKeep gene_dependency ? (y/n) y\n",
      "\tKeep gene_effect ? (y/n) y\n",
      "\tKeep gene_means_proc ? (y/n) y\n",
      "\tKeep hp_data_comb ? (y/n) y\n",
      "\tKeep gene_SDs_proc ? (y/n) y\n",
      "Uploading seed_SDs_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Conversion in progress, line 8250\n",
      "\t Conversion in progress, line 12250\n",
      "\t Conversion in progress, line 15000\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: seed_SDs_proc properly converted and uploaded\n",
      "Uploading seed_means_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 8001)\n",
      "\t Conversion in progress, line 1750\n",
      "\t Conversion in progress, line 5750\n",
      "\t Conversion in progress, line 10250\n",
      "\t Conversion in progress, line 14000\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: seed_means_proc properly converted and uploaded\n",
      "Creating the new version with these files:\n",
      "\tNEW: seed_SDs_proc - NumericMatrixCSV\n",
      "\tNEW: seed_means_proc - NumericMatrixCSV\n",
      "\tKEEP: CL_data_comb - HDF5\n",
      "\tKEEP: gene_fdr - HDF5\n",
      "\tKEEP: pan_dependent_genes - Columnar\n",
      "\tKEEP: gene_dependency - HDF5\n",
      "\tKEEP: gene_effect - HDF5\n",
      "\tKEEP: gene_means_proc - HDF5\n",
      "\tKEEP: hp_data_comb - HDF5\n",
      "\tKEEP: gene_SDs_proc - HDF5\n",
      "\n",
      "Dataset version with id 622ffe4ab8ed43219027ade0c589e33c created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/622ffe4ab8ed43219027ade0c589e33c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'622ffe4ab8ed43219027ade0c589e33c'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_processed_gene_data_D2(DRIVE_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff)\n",
    "prepare_D2_outputs(DRIVE_model_dir)\n",
    "\n",
    "c.update_dataset(dataset_permaname=DRIVE_dataset_id,\n",
    "#     dataset_description=D2_description,\n",
    "#     upload_file_path_dict={os.path.join(DRIVE_model_dir, 'CL_data_comb.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(DRIVE_model_dir, 'hp_data_comb.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(DRIVE_model_dir, 'gene_means_proc.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(DRIVE_model_dir, 'gene_SDs_proc.csv'): 'NumericMatrixCSV'})\n",
    "      upload_file_path_dict={os.path.join(DRIVE_model_dir, 'seed_means_proc.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(DRIVE_model_dir, 'seed_SDs_proc.csv'): 'NumericMatrixCSV'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Marcotte data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 582 genes with all NAs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:41: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:50: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:50: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:51: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:51: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 2566 genes with all poor hp data\n",
      "Now choosing the datasets you would want to keep or remove:\n",
      "\tKeep seed_SDs_proc ? (y/n) n\n",
      "\tNot keeping seed_SDs_proc\n",
      "\tKeep gene_dependency ? (y/n) n\n",
      "\tNot keeping gene_dependency\n",
      "\tKeep hp_data_comb ? (y/n) n\n",
      "\tNot keeping hp_data_comb\n",
      "\tKeep pan_dependent_genes ? (y/n) n\n",
      "\tNot keeping pan_dependent_genes\n",
      "\tKeep gene_means_proc ? (y/n) n\n",
      "\tNot keeping gene_means_proc\n",
      "\tKeep seed_means_proc ? (y/n) n\n",
      "\tNot keeping seed_means_proc\n",
      "\tKeep gene_SDs_proc ? (y/n) n\n",
      "\tNot keeping gene_SDs_proc\n",
      "\tKeep gene_effect ? (y/n) n\n",
      "\tNot keeping gene_effect\n",
      "\tKeep gene_fdr ? (y/n) n\n",
      "\tNot keeping gene_fdr\n",
      "\tKeep CL_data_comb ? (y/n) n\n",
      "\tNot keeping CL_data_comb\n",
      "Uploading gene_means_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Conversion in progress, line 9000\n",
      "\n",
      "\t Done: gene_means_proc properly converted and uploaded\n",
      "Uploading CL_data_comb...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\n",
      "\t Done: CL_data_comb properly converted and uploaded\n",
      "Uploading gene_SDs_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Conversion in progress, line 12750\n",
      "\n",
      "\t Done: gene_SDs_proc properly converted and uploaded\n",
      "Uploading hp_data_comb...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: hp_data_comb properly converted and uploaded\n",
      "Uploading seed_SDs_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Conversion in progress, line 12750\n",
      "\n",
      "\t Done: seed_SDs_proc properly converted and uploaded\n",
      "Uploading seed_means_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Conversion in progress, line 750\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: seed_means_proc properly converted and uploaded\n",
      "Creating the new version with these files:\n",
      "\tNEW: gene_means_proc - NumericMatrixCSV\n",
      "\tNEW: CL_data_comb - NumericMatrixCSV\n",
      "\tNEW: gene_SDs_proc - NumericMatrixCSV\n",
      "\tNEW: hp_data_comb - NumericMatrixCSV\n",
      "\tNEW: seed_SDs_proc - NumericMatrixCSV\n",
      "\tNEW: seed_means_proc - NumericMatrixCSV\n",
      "\n",
      "Dataset version with id db828505cb9f48909aed86f69e1912dd created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/db828505cb9f48909aed86f69e1912dd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'db828505cb9f48909aed86f69e1912dd'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_processed_gene_data_D2(Marc_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff)\n",
    "prepare_D2_outputs(Marc_model_dir)\n",
    "\n",
    "c.update_dataset(dataset_permaname=Marc_dataset_id,\n",
    "    dataset_description=D2_description,\n",
    "    upload_file_path_dict={os.path.join(Marc_model_dir, 'CL_data_comb.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(Marc_model_dir, 'hp_data_comb.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(Marc_model_dir, 'gene_means_proc.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(Marc_model_dir, 'gene_SDs_proc.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(Marc_model_dir, 'seed_means_proc.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(Marc_model_dir, 'seed_SDs_proc.csv'): 'NumericMatrixCSV'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2869: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 414 genes with all NAs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:41: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 828 genes with all poor hp data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:50: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:50: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:51: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/jmmcfarl/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:51: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now choosing the datasets you would want to keep or remove:\n",
      "\tKeep seed_means_proc ? (y/n) n\n",
      "\tNot keeping seed_means_proc\n",
      "\tKeep gene_effect ? (y/n) n\n",
      "\tNot keeping gene_effect\n",
      "\tKeep CL_data_comb ? (y/n) n\n",
      "\tNot keeping CL_data_comb\n",
      "\tKeep gene_means_proc ? (y/n) n\n",
      "\tNot keeping gene_means_proc\n",
      "\tKeep gene_SDs_proc ? (y/n) n\n",
      "\tNot keeping gene_SDs_proc\n",
      "\tKeep hp_data_comb ? (y/n) n\n",
      "\tNot keeping hp_data_comb\n",
      "\tKeep gene_fdr ? (y/n) n\n",
      "\tNot keeping gene_fdr\n",
      "\tKeep seed_SDs_proc ? (y/n) n\n",
      "\tNot keeping seed_SDs_proc\n",
      "\tKeep pan_dependent_genes ? (y/n) n\n",
      "\tNot keeping pan_dependent_genes\n",
      "\tKeep gene_dependency ? (y/n) n\n",
      "\tNot keeping gene_dependency\n",
      "Uploading gene_means_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 2001)\n",
      "\t Scanning through file to determine size (line 10001)\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 3000\n",
      "\t Conversion in progress, line 5000\n",
      "\t Conversion in progress, line 7250\n",
      "\t Conversion in progress, line 9750\n",
      "\t Conversion in progress, line 12000\n",
      "\t Conversion in progress, line 14000\n",
      "\t Conversion in progress, line 16000\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: gene_means_proc properly converted and uploaded\n",
      "Uploading CL_data_comb...\n",
      "Conversion and upload...:\n",
      "\t Conversion in progress, line 500\n",
      "\n",
      "\t Done: CL_data_comb properly converted and uploaded\n",
      "Uploading gene_SDs_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 3001)\n",
      "\t Scanning through file to determine size (line 11001)\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 3000\n",
      "\t Conversion in progress, line 6000\n",
      "\t Conversion in progress, line 8000\n",
      "\t Conversion in progress, line 10000\n",
      "\t Conversion in progress, line 13000\n",
      "\t Conversion in progress, line 15250\n",
      "\t Conversion in progress, line 17250\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: gene_SDs_proc properly converted and uploaded\n",
      "Uploading hp_data_comb...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Conversion in progress, line 55000\n",
      "\t Conversion in progress, line 159500\n",
      "\t Conversion in progress, line 224500\n",
      "\n",
      "\t Done: hp_data_comb properly converted and uploaded\n",
      "Uploading seed_SDs_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 2001)\n",
      "\t Scanning through file to determine size (line 10001)\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 2750\n",
      "\t Conversion in progress, line 4750\n",
      "\t Conversion in progress, line 7000\n",
      "\t Conversion in progress, line 9500\n",
      "\t Conversion in progress, line 11750\n",
      "\t Conversion in progress, line 13750\n",
      "\t Conversion in progress, line 15250\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: seed_SDs_proc properly converted and uploaded\n",
      "Uploading seed_means_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 8001)\n",
      "\t Conversion in progress, line 7500\n",
      "\t Conversion in progress, line 9750\n",
      "\t Conversion in progress, line 11750\n",
      "\t Conversion in progress, line 13750\n",
      "\t Conversion in progress, line 15250\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: seed_means_proc properly converted and uploaded\n",
      "Creating the new version with these files:\n",
      "\tNEW: gene_means_proc - NumericMatrixCSV\n",
      "\tNEW: CL_data_comb - NumericMatrixCSV\n",
      "\tNEW: gene_SDs_proc - NumericMatrixCSV\n",
      "\tNEW: hp_data_comb - NumericMatrixCSV\n",
      "\tNEW: seed_SDs_proc - NumericMatrixCSV\n",
      "\tNEW: seed_means_proc - NumericMatrixCSV\n",
      "\n",
      "Dataset version with id 7df2477aefde405b95d343e63521080f created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/7df2477aefde405b95d343e63521080f\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'7df2477aefde405b95d343e63521080f'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_processed_gene_data_D2(comb_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff)\n",
    "prepare_D2_outputs(comb_model_dir)\n",
    "\n",
    "c.update_dataset(dataset_permaname=comb_dataset_id,\n",
    "    dataset_description=D2_description,\n",
    "    upload_file_path_dict={os.path.join(comb_model_dir, 'CL_data_comb.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(comb_model_dir, 'hp_data_comb.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(comb_model_dir, 'gene_means_proc.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(comb_model_dir, 'gene_SDs_proc.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(comb_model_dir, 'seed_means_proc.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(comb_model_dir, 'seed_SDs_proc.csv'): 'NumericMatrixCSV'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
